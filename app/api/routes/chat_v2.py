"""
Conversational chat endpoint using LangGraph.

This is the new endpoint that uses the conversational graph
for multi-turn conversations with memory and checkpointing.
"""

import json
from typing import Optional

import structlog
from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

from app.agents.conversational_graph.graph import invoke_conversation
from app.api.deps import verify_api_key
from app.core.config import settings
from app.services.conversation_service import conversation_service
from app.services.hitl_service import hitl_service

logger = structlog.get_logger(__name__)

router = APIRouter(prefix="/chat/v2", tags=["chat-v2"])


class ConversationalChatRequest(BaseModel):
    """Conversational chat request."""
    message: str = Field(..., min_length=1, max_length=10000)
    user_id: str = Field(..., min_length=1, description="User ID for memory retrieval")
    conversation_id: str = Field(..., min_length=1, description="Conversation ID generated by frontend")
    context: Optional[dict] = None


class ConversationalChatResponse(BaseModel):
    """Conversational chat response."""
    response: str
    conversation_id: str
    thread_id: str
    artifacts: list[dict] = []
    suggestions: list[str] = []
    hitl_request: Optional[dict] = None
    metadata: dict = {}


@router.post("", response_model=ConversationalChatResponse)
async def conversational_chat(
    request: ConversationalChatRequest,
    _api_key: str = Depends(verify_api_key),
) -> ConversationalChatResponse:
    """
    Conversational chat endpoint using LangGraph.
    
    Features:
    - Multi-turn conversations with memory
    - State persistence via checkpointing
    - Intent classification and routing
    - Multi-platform artifact generation
    - Human-in-the-loop workflows
    - Guardrails for content safety
    
    Args:
        request: Chat request with message and optional conversation_id
        user_id: Current user ID (from auth)
    
    Returns:
        Chat response with artifacts, suggestions, and HITL requests
    """
    user_id = request.user_id

    logger.info(
        "Conversational chat request",
        user_id=user_id,
        conversation_id=request.conversation_id,
        message_length=len(request.message),
    )

    try:
        # Get or create conversation using frontend-provided ID
        conversation = await conversation_service.get_or_create_conversation(
            conversation_id=request.conversation_id,
            user_id=user_id,
        )
        conversation_id = conversation.id
        thread_id = conversation.thread_id

        # Invoke conversational graph
        result_state = await invoke_conversation(
            conversation_id=conversation_id,
            user_id=user_id,
            thread_id=thread_id,
            user_input=request.message,
            config=request.context,
        )
        
        # Extract response
        final_response = result_state.get("final_response", "I'm processing your request.")
        suggestions = result_state.get("suggestions", [])
        artifacts = result_state.get("artifacts", [])
        
        # Check for pending HITL requests
        hitl_requests = await hitl_service.get_pending_requests(conversation_id)
        hitl_request = hitl_requests[0].to_dict() if hitl_requests else None
        
        # Build metadata
        metadata = {
            "intent": result_state.get("current_intent"),
            "nodes_executed": [trace.get("node") for trace in result_state.get("execution_trace", [])],
            "tokens_used": result_state.get("total_tokens_used", 0),
            "cost": result_state.get("total_cost", 0.0),
            "guardrail_passed": result_state.get("guardrail_passed", True),
        }
        
        return ConversationalChatResponse(
            response=final_response,
            conversation_id=conversation_id,
            thread_id=thread_id,
            artifacts=[
                {
                    "id": a.get("id"),
                    "type": a.get("artifact_type"),
                    "platform": a.get("platform"),
                    "content": a.get("content"),
                }
                for a in artifacts
            ],
            suggestions=suggestions,
            hitl_request=hitl_request,
            metadata=metadata,
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error("Conversational chat failed", error=str(e), user_id=user_id)
        detail = str(e) if settings.environment == "development" else "Internal server error"
        raise HTTPException(status_code=500, detail=detail)


@router.post("/stream")
async def conversational_chat_stream(
    request: ConversationalChatRequest,
    _api_key: str = Depends(verify_api_key),
):
    """
    Streaming chat endpoint using Server-Sent Events (SSE).

    Returns text/event-stream with events:
    - event: step     — pipeline stage progress (4 stages)
    - event: token    — LLM output chunk (for QA responses)
    - event: artifact — generated content artifact
    - event: done     — final result with full response
    - event: error    — error occurred
    """
    import asyncio
    from app.agents.conversational_graph.graph import stream_conversation_sse
    from app.agents.conversational_graph.event_bus import EventBus

    user_id = request.user_id

    logger.info(
        "Streaming chat request",
        user_id=user_id,
        conversation_id=request.conversation_id,
        message_length=len(request.message),
    )

    # Get or create conversation using frontend-provided ID
    try:
        conversation = await conversation_service.get_or_create_conversation(
            conversation_id=request.conversation_id,
            user_id=user_id,
        )
        conversation_id = conversation.id
        thread_id = conversation.thread_id
    except Exception as e:
        logger.error("Failed to setup conversation for streaming", error=str(e))
        detail = str(e) if settings.environment == "development" else "Internal server error"
        raise HTTPException(status_code=500, detail=detail)

    event_bus = EventBus()

    async def sse_generator():
        # Run graph in background, pushing events to event_bus
        graph_task = asyncio.create_task(
            stream_conversation_sse(
                conversation_id=conversation_id,
                user_id=user_id,
                thread_id=thread_id,
                user_input=request.message,
                event_bus=event_bus,
                config=request.context,
            )
        )

        try:
            # Yield events as they arrive from the bus
            async for evt in event_bus:
                yield f"event: {evt.event}\ndata: {json.dumps(evt.data)}\n\n"

            # Graph finished — get result
            final_state = await graph_task

            # Send final "done" event
            done_data = {
                "response": (final_state or {}).get("final_response", ""),
                "conversation_id": conversation_id,
                "thread_id": thread_id,
                "artifacts": (final_state or {}).get("artifacts", []),
                "suggestions": (final_state or {}).get("suggestions", []),
                "requires_approval": (final_state or {}).get(
                    "requires_approval", False
                ),
            }
            yield f"event: done\ndata: {json.dumps(done_data)}\n\n"

        except Exception as e:
            logger.error("SSE streaming error", error=str(e))
            error_data = {"message": str(e)}
            yield f"event: error\ndata: {json.dumps(error_data)}\n\n"

    return StreamingResponse(
        sse_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )


class HITLResponseRequest(BaseModel):
    """HITL response request."""
    request_id: str
    response: str
    user_id: str
    selected_options: Optional[list] = None


@router.post("/respond-hitl")
async def respond_to_hitl(
    request: HITLResponseRequest,
    _api_key: str = Depends(verify_api_key),
) -> dict:
    """
    Respond to a HITL request and resume conversation.
    
    Args:
        request_id: HITL request identifier
        response: User's response
        selected_options: Selected option IDs
        user_id: Current user ID (from auth)
    
    Returns:
        Updated HITL request
    """
    try:
        # Get HITL request
        hitl_request = await hitl_service.get_request(request.request_id)

        if not hitl_request:
            raise HTTPException(status_code=404, detail="HITL request not found")

        # Verify ownership
        conversation = await conversation_service.get_conversation(
            hitl_request.conversation_id
        )

        if not conversation or conversation.user_id != request.user_id:
            raise HTTPException(status_code=403, detail="Access denied")

        # Respond to request
        updated_request = await hitl_service.respond_to_request(
            request_id=request.request_id,
            response=request.response,
            selected_options=request.selected_options,
            action="approve",
        )

        # TODO: Resume conversation with user response
        # This would involve re-invoking the graph with the HITL context

        return updated_request.to_dict()

    except HTTPException:
        raise
    except Exception as e:
        logger.error("HITL response failed", error=str(e), request_id=request.request_id)
        detail = str(e) if settings.environment == "development" else "Internal server error"
        raise HTTPException(status_code=500, detail=detail)
